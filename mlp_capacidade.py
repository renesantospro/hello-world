# -*- coding: utf-8 -*-
"""MLP_Capacidade.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IwlKfuJaf7gwtAh6bkb95lbz7HeXS1JF

# Capacidade MLP

### Função auxiliar para plot da fronteira de decisão do classificador.
"""

def plot_boundary(X, y, model, size):
  x_min, x_max = X[:, 0].min()-0.1, X[:, 0].max()+0.1
  y_min, y_max = X[:, 1].min()-0.1, X[:, 1].max()+0.1
  
  spacing = min(x_max - x_min, y_max - y_min) / 100
  
  XX, YY = np.meshgrid(np.arange(x_min, x_max, spacing),
                       np.arange(y_min, y_max, spacing))
  
  data = np.hstack((XX.ravel().reshape(-1,1), 
                    YY.ravel().reshape(-1,1)))
  
  db_prob = model(Variable(torch.Tensor(data)).cuda() )
  clf = np.where(db_prob.cpu().data   < 0.5,0,1)
  Z = clf.reshape(XX.shape)
  
  plt.figure()
  plt.contourf(XX, YY, Z, cmap=plt.cm.bwr, alpha=0.5)
  plt.scatter(X[:,0], X[:,1], c=y[:,0], edgecolors='k', s=size)

"""### Função para treinamento do modelo e visualização dos resultados."""

def train(X, Y, size=60):
  
  p = MLP().cuda()

  Xt = Variable(torch.Tensor(X)).cuda()
  Yt = Variable(torch.Tensor(Y)).cuda()

  criterion = nn.L1Loss().cuda()
  optimizer = torch.optim.Adam(p.parameters(), lr=1e-2)

  num_epochs = 1000
  for epoch in range(num_epochs):

    p.train()

    output = p(Xt)
    loss = criterion(output, Yt)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()


  out = p(Xt)
  plot_boundary(X, Y, p, size)

"""# Atividade

Altere a capacidade do MLP na última célula para suportar o problema de classificação apresentado a seguir.
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.datasets import make_circles
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline

X, y = make_circles(200,noise=0.2, factor=0.5, random_state=1)
plt.scatter(X[:,0], X[:,1], c=y, cmap='winter')

from torch.autograd import Variable
import torch.nn as nn
import torch

class MLP(nn.Module):

  def __init__(self):
    super(MLP, self).__init__()
    
    # nn.Linear(dimensão da entrada, dimensão da saída)
    self.linear_hidden = nn.Linear(2, 3)
    self.linear_out    = nn.Linear(3, 1)
    self.activation    = nn.Sigmoid()
    
  def forward(self, x):
    feature = self.activation(self.linear_hidden(x))
    output  = self.activation(self.linear_out(feature))
    
    return output
  
  
train(X, np.expand_dims(y, axis=-1))